<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Intuitive Introduction for Attention Models - An intuitive introduction to attention models in RNN seq2seq models">
    <title>Intuitive Introduction for Attention Models - Mina Rafla</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <!-- Header loaded dynamically -->
    <div id="header-placeholder"></div>

    <!-- Main Content -->
    <main class="main-content">
        <div class="container">
            <article class="blog-article">
                <!-- Back to blog link -->
                <nav class="breadcrumb">
                    <a href="../../blog.html" class="breadcrumb-link">← Back to Blog</a>
                </nav>

                <!-- Article Header -->
                <header class="article-header">
                    <h1 class="article-title">Intuitive Introduction for Attention Models</h1>
                    <p class="article-meta">Published: <time datetime="2025-12-08">December 8, 2025</time></p>
                </header>

                <!-- Article Content -->
                <div class="article-content">
<p>Hi there, in this post, my notes for the "Attention for RNN Seq2Seq models by Shusen Wang" combined with other information from other resources I found on the internet. I put my references at the end.</p><h2 id="revisiting-seq2seq-models-why-do-we-need-attention">Revisiting seq2seq models: why do we need attention</h2><p>Assuming we want to translate a german sentence (which is our sequence) to english sentence. A seq2seq model will be composed of our decoder and encoder, each of them is a RNN model.</p><figure><img src="images/seq2seq_model.png" alt="" /><figcaption>Sequence-to-sequence model architecture</figcaption></figure><p>The two sentences can have different lengths. The input of the encoder RNN is a set of vectors, each vector is the embedding of a word from the source language (in our case german language).</p><p>Each vector <span class="math inline"><em>h</em><sub><em>i</em></sub></span> is the summary of all the precedents inputs. The final state <span class="math inline"><em>h</em><sub><em>m</em></sub></span> is a condensed representation of all the input sentence. Ideally, it contains all the needed information for the translation.</p><p>The input of the decoder RNN is the final state of the encoder <span class="math inline"><em>h</em><sub><em>m</em></sub></span>. The decoder network acts a text generator. At each step, it generates a word in the target language. And the generated word becomes the next input <span class="math inline"><em>x</em>′<sub>1</sub>, …, <em>x</em>′<sub><em>t</em></sub></span>.</p><p>We can think of the difference between <span class="math inline"><em>x</em>′<sub><em>t</em></sub></span> and <span class="math inline"><em>s</em><sub><em>t</em></sub></span>: <span class="math inline"><em>x</em>′<sub><em>t</em></sub></span> is what the decoder reads (which during inference time is the generated word) and <span class="math inline"><em>s</em><sub><em>t</em></sub></span> is what the decoder thinks after reading it.</p><p>A symbol here represents a RNN, but it can also be an LSTM or a GRU. LSTM and GRU are some RNN improvements, that was created to improve the memory of RNN, but they are not as efficient as attention models.</p><p>The decoder doesn’t directly see the source language. Shortcoming: the final state of the encoder is incapable of remembering long sequences, because <span class="math inline"><em>h</em><sub><em>m</em></sub></span> will not have enough capacity to remember long sequences.</p><p>That’s why we need attention models.</p><h2 id="attention-models">Attention models</h2><p>Attention mechanism allows the decoder to know where to look whenever it tries to translate a word. In simple words, as we will see now, at each step of the translation, it gives for each of the encoders’ states a weight depending on the word it’s about to translate (depending on the decoder’s current state). This weight can be like a similarity function between the current state <span class="math inline"><em>s</em><sub><em>t</em></sub></span> and all the encoder’s hidden states <span class="math inline"><em>h</em><sub>1</sub>, …, <em>h</em><sub><em>m</em></sub></span>, also known as ‘align function’.</p><p>In this post, I will not be going into the details of how the align function is calculated. All the weights of each step add up to 1.</p><figure><img src="images/weights_attention1.png" alt="" /><figcaption>Calculating weights for <span class="math inline"><em>S</em><sub>0</sub></span></figcaption></figure><p>Using the weights we calculate a weighted average of them, that leads to a context vector <span class="math inline"><em>c</em></span>.</p><p><img src="images/context_vectors.png" alt="Context vecotr" /> This time, the decoder to calculate a new state vector, instead of using only the new input <span class="math inline"><em>x</em>′</span> and the previous state, it also uses the context vector of the current step. The context vector is the weighted average of all the hidden state of the encoder, so now the decoder knows where to focus in the encoder to perform the translation task.</p><p><br /><span class="math display"><em>c</em><sub>0</sub> = <em>α</em><sub>1</sub> ⋅ <em>h</em><sub>1</sub> + <em>α</em><sub>2</sub> ⋅ <em>h</em><sub>2</sub> + … + <em>α</em><sub><em>m</em></sub> ⋅ <em>h</em><sub><em>m</em></sub></span><br /></p><p>Then <span class="math inline"><em>s</em><sub>0</sub></span> takes as input the current decoder’s input, the previous state <span class="math inline"><em>s</em></span> and the previous context vector.</p><p>For example: <br /><span class="math display"><em>S</em><sub>2</sub> = tanh (<em>A</em>′ ⋅ [<em>x</em>′<sub>2</sub>, <em>s</em><sub>1</sub>, <em>c</em><sub>1</sub>])</span><br /></p><p>A question to challenge your understanding: What is the total number of <span class="math inline"><em>α</em></span> values we have to compute?</p><p>Answer: At each step we compute <span class="math inline"><em>m</em></span> <span class="math inline"><em>α</em></span> values, so in total we will have <span class="math inline"><em>t</em> ⋅ <em>m</em></span> <span class="math inline"><em>α</em></span> values.</p><h2 id="references">References</h2><ul><li>https://www.youtube.com/watch?v=B3uws4cLcFw&amp;list=PLgtf4d9zHHO8p_zDKstvqvtkv80jhHxoE</li><li>https://superstudy.guide/transformers-large-language-models/</li></ul>

                </div>
            </article>
        </div>
    </main>

    <!-- Footer loaded dynamically -->
    <div id="footer-placeholder"></div>

    <script src="../../data/cv-data.js"></script>
    <script src="../../js/script.js"></script>
</body>
</html>

