<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Intuitive Introduction for Attention Models - An intuitive introduction to attention models in RNN seq2seq models">
    <title>Intuitive Introduction for Attention Models</title>
    <link rel="stylesheet" href="../../styles.css">
    
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
</head>
<body>
    <!-- Header loaded dynamically -->
    <div id="header-placeholder"></div>

    <!-- Main Content -->
    <main class="main-content">
        <div class="container">
            <article class="blog-article">
                <!-- Back to blog link -->
                <nav class="breadcrumb">
                    <a href="../../blog.html" class="breadcrumb-link">← Back to Blog</a>
                </nav>

                <!-- Article Header -->
                <header class="article-header">
                    <h1 class="article-title">Intuitive Introduction for Attention Models</h1>
                    <p class="article-meta">Published: <time datetime="2025-12-08">December 8, 2025</time></p>
                </header>


                <!-- Article Content -->
                <div class="article-content">
<h1 id="intuitive-introduction-for-attention-models">Intuitive introduction for attention models</h1><p>Hi there, in this post, my notes for the “Attention for RNN Seq2Seq models by Shusen Wang” combined with other information from other resources I found on the internet. I put my references at the end.</p><h2 id="revisiting-seq2seq-models-why-do-we-need-attention">Revisiting seq2seq models: why do we need attention</h2><p>Assuming we want to translate a german sentence (which is our sequence) to english sentence. A seq2seq model will be composed of our decoder and encoder, each of them is a RNN model.</p><figure><img src="images/seq2seq_model.png" alt="" /><figcaption>Sequence-to-sequence model architecture</figcaption></figure><p>The two sentences can have different lengths. The input of the encoder RNN is a set of vectors, each vector is the embedding of a word from the source language (in our case german language).</p><p>Each vector <span class="math inline"><em>h</em><sub><em>i</em></sub></span> is the summary of all the precedents inputs. The final state <span class="math inline"><em>h</em><sub><em>m</em></sub></span> is a condensed representation of all the input sentence. Ideally, it contains all the needed information for the translation.</p><p>The input of the decoder RNN is the final state of the encoder <span class="math inline"><em>h</em><sub><em>m</em></sub></span>. The decoder network acts a text generator. At each step, it generates a word in the target language. And the generated word becomes the next input <span class="math inline"><em>x</em>′<sub>1</sub>, …, <em>x</em>′<sub><em>t</em></sub></span>.</p><p>We can think of the difference between <span class="math inline"><em>x</em>′<sub><em>t</em></sub></span> and <span class="math inline"><em>s</em><sub><em>t</em></sub></span>: <span class="math inline"><em>x</em>′<sub><em>t</em></sub></span> is what the decoder reads (which during inference time is the generated word) and <span class="math inline"><em>s</em><sub><em>t</em></sub></span> is what the decoder thinks after reading it.</p><p><span class="math inline"><em>A</em></span> symbol here represents a RNN, but it can also be an LSTM or a GRU. LSTM and GRU are some RNN improvements, that was created to improve the memory of RNN, but they are not as efficient as attention models.</p><p>The decoder doesn’t directly see the source language.</p><p><strong><u>Shortcoming:</u> the final state of the encoder is incapable of remembering long sequences, because <span class="math inline"><em>h</em><sub><em>m</em></sub></span> will not have enough capacity to remember long sequences.</strong></p><p>That’s why we need attention models.</p><h2 id="attention-models">Attention models</h2><p>Attention mechanism allows the decoder to know where to look whenever it tries to translate a word. In simple words, as we will see now, at each step of the translation (the decoder part), it gives for each of the encoders’ states a weight depending on the word it’s about to translate (depending on the decoder’s current state). This weight is like a similarity function between the current state <span class="math inline"><em>s</em><sub><em>t</em></sub></span> and all the encoder’s hidden states <span class="math inline"><em>h</em><sub>1</sub>, …, <em>h</em><sub><em>m</em></sub></span>, also known as ‘align function’.</p><p>I will briefly explain the align function at the end of the post. But for information, in the transformers “language”, when we want to compare the current state <span class="math inline"><em>s</em><sub><em>t</em></sub></span> of the decoder with the encoder’s hidden states, we call <span class="math inline"><em>s</em><sub><em>t</em></sub></span> the <u>query</u>, and <span class="math inline"><em>h</em><sub>1</sub>, …, <em>h</em><sub><em>m</em></sub></span> are called the <u>keys</u>.</p><p>All the weights of each step add up to 1.</p><figure><img src="images/weights_attention1.png" alt="" /><figcaption>Calculating weights for <span class="math inline"><em>S</em><sub>0</sub></span></figcaption></figure><p>Using the weights we calculate a weighted average of them, that leads to a context vector <span class="math inline"><em>c</em></span>.</p><p><img src="images/context_vectors.png" alt="Context vecotr" /> This time, the decoder to calculate a new state vector, instead of using only the new input <span class="math inline"><em>x</em>′</span> and the previous state, it also uses the context vector of the current step. The context vector is the weighted average of all the hidden state of the encoder, so now the decoder knows where to focus in the encoder to perform the translation task.</p><p><br /><span class="math display"><em>c</em><sub>0</sub> = <em>α</em>1 ⋅ <em>h</em><sub>1</sub> + <em>α</em>2 ⋅ <em>h</em><sub>2</sub> + … + <em>α</em><em>m</em> ⋅ <em>h</em><sub><em>m</em></sub></span><br /></p><p>Then <span class="math inline"><em>s</em><sub>0</sub></span> takes as input the current decoder’s input, the previous state <span class="math inline"><em>s</em></span> and the previous context vector.</p><p>For example: <br /><span class="math display"><em>c</em><sub>1</sub> = <em>α</em>1 ⋅ <em>h</em><sub>1</sub> + <em>α</em>2 ⋅ <em>h</em><sub>2</sub> + … + <em>α</em><em>m</em> ⋅ <em>h</em><sub><em>m</em></sub></span><br /> <br /><span class="math display"><em>S</em><sub>2</sub> = tanh (<em>A</em>′ ⋅ [<em>x</em>′<sub>2</sub>, <em>s</em><sub>1</sub>, <em>c</em><sub>1</sub>])</span><br /></p><p><strong>Note:</strong> The attention weights (α₁, α₂, …, αₘ) used to calculate <span class="math inline"><em>c</em><sub>1</sub></span> are <strong>not the same</strong> as the attention weights used to calculate <span class="math inline"><em>c</em><sub>0</sub></span>. I used the same symbols (α) for simplification. However, at each step, <strong>new attention weights</strong> are calculated using the align function between the current decoder state <span class="math inline"><em>s</em><sub><em>t</em></sub></span> and all the hidden states of the encoder (<span class="math inline"><em>h</em><sub>1</sub></span> to <span class="math inline"><em>h</em><sub><em>m</em></sub></span>).</p><p>A question to challenge your understanding: What is the total number of attention weights (α values) we have to compute?</p><p>Answer: At each step we compute <span class="math inline"><em>m</em></span> attention weights (α₁, α₂, …, αₘ), so in total we will have <span class="math inline"><em>t</em> ⋅ <em>m</em></span> attention weights, where <span class="math inline"><em>t</em></span> is the number of decoder steps and <span class="math inline"><em>m</em></span> is the number of encoder hidden states.</p><h2 id="the-align-function">The align function</h2><p>The align function can be calculated using several ways. The most popular way and the one used in the transformers is the following way.</p><p>As mentioned, the vector that comes from the current state of the decoder is called the query, and each of the vectors of the hidden states of the hidden state of the encoder <span class="math inline"><em>h</em><sub><em>i</em></sub></span> is called the key.</p><p>Here is how the attention (align) function typically works, step by step:</p><ol type="1"><li><strong>Compute Query and Keys:</strong><ul><li><p>For each decoder state <span class="math inline"><em>s</em><sub><em>t</em></sub></span>, compute a <strong>query vector</strong>:<br />
<br /><span class="math display"><em>q</em><sub><em>t</em></sub> = <em>W</em><sub><em>Q</em></sub> ⋅ <em>s</em><sub><em>t</em></sub></span><br /></p></li><li><p>For each encoder hidden state <span class="math inline"><em>h</em><sub><em>i</em></sub></span>, compute a <strong>key vector</strong>:<br />
<br /><span class="math display"><em>k</em><sub><em>i</em></sub> = <em>W</em><sub><em>K</em></sub> ⋅ <em>h</em><sub><em>i</em></sub></span><br /></p><p><strong>Little algebra reminder</strong>: A matrix vector product is a vector.</p><p><span class="math inline"><em>W</em><sub><em>Q</em></sub></span> and <span class="math inline"><em>W</em><sub><em>K</em></sub></span> are two parameter matrices learnt from the data.</p></li></ul></li><li><strong>Calculate Similarity Scores:</strong><ul><li><p>For each encoder hidden state, calculate a score by taking the dot product between the query and each key:</p><p><strong>Equation:</strong><br />
<br /><span class="math display">score<sub><em>i</em></sub> = <strong>q</strong><sub><em>t</em></sub> ⋅ <strong>k</strong><sub><em>i</em></sub></span><br /></p></li></ul></li><li><strong>(Optional) Scale the Scores:</strong><ul><li><p>Sometimes, to avoid very large values, divide the score by the square root of the dimensionality of the key vectors (<span class="math inline"><em>d</em><sub><em>k</em></sub></span>):</p><p><br /><span class="math display">$$\text{score}_i = \frac{\mathbf{q}_t \cdot \mathbf{k}_i}{\sqrt{d_k}}$$</span><br /></p></li></ul></li><li><strong>Apply Softmax to Get Attention Weights:</strong><ul><li><p>Pass all the scores through a softmax function to turn them into probabilities (attention weights <span class="math inline"><em>α</em><sub><em>i</em></sub></span>):</p><p><br /><span class="math display">[<em>α</em>1...<em>α</em><em>m</em>] = <em>s</em><em>o</em><em>f</em><em>t</em><em>m</em><em>a</em><em>x</em>([score<sub>1</sub>..score<sub><em>m</em></sub>])</span><br /></p></li></ul></li><li><strong>Compute the Context Vector:</strong><ul><li>Take a weighted sum of the encoder hidden states (<span class="math inline"><em>h</em><sub><em>i</em></sub></span>) using their attention weights:<br />
<br /><span class="math display"><em>c</em>_<em>t</em> = ∑<sub><em>i</em></sub><em>α</em><sub><em>i</em></sub> ⋅ <em>h</em><sub><em>i</em></sub></span><br /></li></ul></li><li><strong>Use the Context Vector:</strong><ul><li>The decoder then uses this context vector <span class="math inline"><em>c</em><sub><em>t</em></sub></span> (along with other inputs) to generate the next word in the sequence.</li></ul></li></ol><h2 id="references">References</h2><ul><li>https://www.youtube.com/watch?v=B3uws4cLcFw&amp;list=PLgtf4d9zHHO8p_zDKstvqvtkv80jhHxoE</li><li>https://superstudy.guide/transformers-large-language-models/</li></ul>

                </div>
            </article>
        </div>
    </main>

    <!-- Footer loaded dynamically -->
    <div id="footer-placeholder"></div>

    <script src="../../data/cv-data.js"></script>
    <script src="../../js/script.js"></script>
    
    <!-- Render math equations with KaTeX -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement !== 'undefined') {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\[', right: '\\]', display: true},
                        {left: '\\(', right: '\\)', display: false}
                    ],
                    throwOnError: false
                });
            }
        });
    </script>
</body>
</html>

